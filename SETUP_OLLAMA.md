# ü¶ô Configuration Ollama - IA Open Source Gratuite

Ollama est une solution **100% gratuite et open source** pour ex√©cuter des mod√®les d'IA localement sur votre machine. C'est la meilleure option pour √©viter les co√ªts des APIs payantes !

## üéØ Pourquoi Ollama ?

‚úÖ **100% Gratuit** - Aucun co√ªt, aucune limite  
‚úÖ **Open Source** - Transparence totale  
‚úÖ **Local** - Vos donn√©es restent sur votre machine (privacy)  
‚úÖ **Performant** - Fonctionne m√™me sans internet  
‚úÖ **Plusieurs mod√®les** - Llama, Mistral, Phi, etc.

---

## üì• Installation d'Ollama

### Windows

1. T√©l√©chargez Ollama depuis : https://ollama.com/download
2. Installez le fichier `.exe` t√©l√©charg√©
3. Ollama d√©marrera automatiquement en arri√®re-plan

### macOS

```bash
brew install ollama
```

Ou t√©l√©chargez depuis : https://ollama.com/download

### Linux

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

---

## üöÄ Premi√®re utilisation

### 1. T√©l√©charger un mod√®le

Ollama a besoin d'un mod√®le pour fonctionner. Voici les meilleurs mod√®les recommand√©s :

**Option 1 : Llama 3.2 (Recommand√© - ~2GB)**
```bash
ollama pull llama3.2
```

**Option 2 : Mistral (Plus petit - ~4GB)**
```bash
ollama pull mistral
```

**Option 3 : Phi-3 (Tr√®s l√©ger - ~2GB)**
```bash
ollama pull phi3
```

**Option 4 : Llama 3.1 (Plus performant - ~4.7GB)**
```bash
ollama pull llama3.1
```

üí° **Conseil** : Commencez par `llama3.2` - c'est un bon √©quilibre entre qualit√© et taille.

### 2. V√©rifier que Ollama fonctionne

```bash
ollama list
```

Vous devriez voir votre mod√®le dans la liste.

### 3. Tester Ollama

```bash
ollama run llama3.2 "Bonjour, comment √ßa va ?"
```

Si √ßa fonctionne, Ollama est pr√™t ! üéâ

---

## ‚öôÔ∏è Configuration dans le projet

### Option 1 : Configuration par d√©faut (Recommand√©)

Si Ollama tourne sur `http://localhost:11434` (par d√©faut), **aucune configuration n'est n√©cessaire** ! Le chatbot d√©tectera automatiquement Ollama.

### Option 2 : Configuration personnalis√©e

Si Ollama tourne sur un autre port ou une autre machine, cr√©ez un fichier `.env` :

```env
# URL de votre instance Ollama
VITE_OLLAMA_BASE_URL=http://localhost:11434

# Mod√®le √† utiliser (par d√©faut: llama3.2)
VITE_OLLAMA_MODEL=llama3.2
```

---

## üß™ Tester l'int√©gration

1. **Assurez-vous qu'Ollama tourne** :
   ```bash
   ollama serve
   ```
   (Normalement, Ollama d√©marre automatiquement)

2. **Lancez le chatbot** :
   ```bash
   npm run dev
   ```

3. **V√©rifiez le message de bienvenue** :
   - Si vous voyez "(Mode IA Ollama open source activ√© ‚ú®)" ‚Üí √áa fonctionne ! ‚úÖ
   - Si vous voyez "(Mode local)" ‚Üí Ollama n'est pas d√©tect√© ‚ö†Ô∏è

4. **Posez une question** et v√©rifiez que la r√©ponse vient bien d'Ollama !

---

## üîß D√©pannage

### Ollama n'est pas d√©tect√©

1. **V√©rifiez qu'Ollama tourne** :
   ```bash
   ollama list
   ```
   Si √ßa ne fonctionne pas, lancez :
   ```bash
   ollama serve
   ```

2. **V√©rifiez l'URL** :
   - Par d√©faut : `http://localhost:11434`
   - Testez dans votre navigateur : http://localhost:11434/api/tags
   - Vous devriez voir une liste de mod√®les en JSON

3. **V√©rifiez les CORS** :
   - Ollama devrait accepter les requ√™tes depuis le navigateur
   - Si vous avez des erreurs CORS, v√©rifiez la configuration d'Ollama

### Le mod√®le n'existe pas

Si vous obtenez une erreur "model not found" :

1. **Listez les mod√®les disponibles** :
   ```bash
   ollama list
   ```

2. **T√©l√©chargez le mod√®le manquant** :
   ```bash
   ollama pull llama3.2
   ```

3. **Mettez √† jour le `.env`** si vous utilisez un autre mod√®le :
   ```env
   VITE_OLLAMA_MODEL=votre-modele
   ```

### Performance lente

- Utilisez un mod√®le plus petit (phi3, llama3.2 au lieu de llama3.1)
- Assurez-vous d'avoir assez de RAM (minimum 8GB recommand√©)
- Fermez les autres applications gourmandes

---

## üìä Comparaison des mod√®les

| Mod√®le | Taille | Qualit√© | Vitesse | RAM requise |
|--------|--------|---------|---------|-------------|
| **llama3.2** | ~2GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö° | 4GB+ |
| **mistral** | ~4GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö° | 8GB+ |
| **phi3** | ~2GB | ‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö° | 4GB+ |
| **llama3.1** | ~4.7GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö° | 8GB+ |

üí° **Recommandation** : Commencez par `llama3.2` pour un bon √©quilibre.

---

## üéØ Avantages vs APIs payantes

| Crit√®re | Ollama | OpenAI | Gemini |
|---------|--------|--------|--------|
| **Co√ªt** | ‚úÖ Gratuit | ‚ùå Payant | ‚ö†Ô∏è Gratuit (quota) |
| **Privacy** | ‚úÖ 100% local | ‚ùå Donn√©es envoy√©es | ‚ùå Donn√©es envoy√©es |
| **Internet** | ‚úÖ Pas besoin | ‚ùå Requis | ‚ùå Requis |
| **Limite** | ‚úÖ Aucune | ‚ùå Quota/co√ªt | ‚ö†Ô∏è Quota |
| **Open Source** | ‚úÖ Oui | ‚ùå Non | ‚ùå Non |

---

## üöÄ Pour aller plus loin

### Utiliser Ollama sur un serveur distant

Si vous voulez utiliser Ollama sur une autre machine :

1. Configurez Ollama pour accepter les connexions distantes
2. Mettez √† jour `.env` :
   ```env
   VITE_OLLAMA_BASE_URL=http://votre-serveur:11434
   ```

### Changer de mod√®le dynamiquement

Vous pouvez changer de mod√®le en modifiant `VITE_OLLAMA_MODEL` dans `.env` et en red√©marrant le serveur.

---

## ‚úÖ Checklist

- [ ] Ollama est install√©
- [ ] Un mod√®le est t√©l√©charg√© (`ollama pull llama3.2`)
- [ ] Ollama tourne (`ollama list` fonctionne)
- [ ] Le chatbot d√©tecte Ollama (message "(Mode IA Ollama activ√©)")
- [ ] Les r√©ponses sont g√©n√©r√©es par Ollama

---

**C'est tout ! Vous avez maintenant une IA 100% gratuite et open source ! üéâ**

Pour plus d'informations sur Ollama : https://ollama.com


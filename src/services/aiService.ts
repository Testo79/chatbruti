/**
 * Service d'int√©gration IA (Ollama open source, OpenAI GPT ou Google Gemini)
 * G√©n√®re des r√©ponses humoristiques mais contextuelles
 */

import OpenAI from 'openai';
import { GoogleGenerativeAI } from '@google/generative-ai';
import type { MoodMode } from '../utils/chatEngine';
import type { AvatarExpression } from '../components/BotAvatar';

export type AIProvider = 'groq' | 'together' | 'huggingface' | 'ollama' | 'openai' | 'gemini' | 'local';

export interface AIResponse {
  text: string;
  expression: AvatarExpression;
}

export interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

// Configuration depuis les variables d'environnement
// Pour la production, utiliser le proxy Node.js sur le port 8080 (avec CORS) sur GCP
const OLLAMA_BASE_URL = import.meta.env.VITE_OLLAMA_BASE_URL || 
  (window.location.hostname !== 'localhost' && window.location.hostname !== '127.0.0.1' 
    ? 'http://173.212.199.62:8080'  // Port 8080 via proxy Node.js CORS sur instance puissante
    : 'http://localhost:11434');
const GROQ_API_KEY = import.meta.env.VITE_GROQ_API_KEY;
const GROQ_MODEL = import.meta.env.VITE_GROQ_MODEL || 'llama-3.1-8b-instant'; // Mod√®le rapide et gratuit
const TOGETHER_API_KEY = import.meta.env.VITE_TOGETHER_API_KEY;
const TOGETHER_MODEL = import.meta.env.VITE_TOGETHER_MODEL || 'meta-llama/Llama-3.2-1B-Instruct';
const HUGGINGFACE_API_KEY = import.meta.env.VITE_HUGGINGFACE_API_KEY;
const HUGGINGFACE_MODEL = import.meta.env.VITE_HUGGINGFACE_MODEL || 'meta-llama/Llama-3.2-1B-Instruct';
const OPENAI_API_KEY = import.meta.env.VITE_OPENAI_API_KEY;
const GEMINI_API_KEY = import.meta.env.VITE_GEMINI_API_KEY;
const OLLAMA_MODEL = import.meta.env.VITE_OLLAMA_MODEL || 'tinyllama'; // Mod√®le par d√©faut (l√©ger et rapide)

// Debug: Afficher la configuration au chargement
console.log('üîß Configuration IA:', {
  OLLAMA_BASE_URL,
  OLLAMA_MODEL,
  hasHuggingFace: !!HUGGINGFACE_API_KEY,
  hasOpenAI: !!OPENAI_API_KEY,
  hasGemini: !!GEMINI_API_KEY,
  huggingFaceModel: HUGGINGFACE_MODEL,
  huggingFaceKeyLength: HUGGINGFACE_API_KEY?.length || 0,
  env: import.meta.env.VITE_OLLAMA_BASE_URL,
  rawEnv: {
    VITE_HUGGINGFACE_API_KEY: import.meta.env.VITE_HUGGINGFACE_API_KEY ? 'PRESENT' : 'MISSING',
    VITE_HUGGINGFACE_MODEL: import.meta.env.VITE_HUGGINGFACE_MODEL || 'NOT_SET'
  }
});

/**
 * V√©rifie si Ollama est disponible
 */
async function checkOllamaAvailable(): Promise<boolean> {
  try {
    console.log(`üîç Tentative de connexion √† Ollama sur: ${OLLAMA_BASE_URL}`);
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 15000); // Timeout de 15 secondes pour le cloud
    
    // Essayer d'abord sans headers personnalis√©s pour √©viter les probl√®mes de preflight
    const response = await fetch(`${OLLAMA_BASE_URL}/api/tags`, {
      method: 'GET',
      signal: controller.signal,
      mode: 'cors', // Important pour les requ√™tes cross-origin
      // Ne pas ajouter de headers personnalis√©s pour √©viter les probl√®mes de preflight CORS
    });
    
    clearTimeout(timeoutId);
    
    if (response.ok) {
      const data = await response.json();
      console.log('‚úÖ Ollama d√©tect√© et disponible !', data);
      return true;
    } else {
      // Lire le body de l'erreur pour plus d'infos
      const errorText = await response.text().catch(() => '');
      console.log(`‚ö†Ô∏è Ollama non disponible (status: ${response.status})`, errorText);
      console.log(`Response headers:`, Object.fromEntries(response.headers.entries()));
      console.log(`Request URL: ${OLLAMA_BASE_URL}/api/tags`);
      return false;
    }
  } catch (error) {
    if (error instanceof Error && error.name === 'AbortError') {
      console.log('‚ö†Ô∏è Ollama: Timeout de connexion (15s)');
    } else if (error instanceof TypeError && error.message.includes('Failed to fetch')) {
      console.log('‚ö†Ô∏è Ollama: Erreur de r√©seau ou CORS. V√©rifiez la configuration Nginx.');
      console.log(`üí° URL test√©e: ${OLLAMA_BASE_URL}/api/tags`);
    } else {
      console.log('‚ö†Ô∏è Ollama non d√©tect√©:', error instanceof Error ? error.message : 'Connexion impossible');
    }
    console.log(`üí° URL test√©e: ${OLLAMA_BASE_URL}`);
    return false;
  }
}

// Cache pour √©viter de v√©rifier Ollama √† chaque fois
let ollamaAvailableCache: boolean | null = null;
let ollamaCheckTime = 0;
const OLLAMA_CHECK_CACHE_DURATION = 60000; // 1 minute

/**
 * D√©tecter si Ollama est configur√© pour le cloud (URL personnalis√©e)
 */
function isOllamaCloudConfigured(): boolean {
  // Si VITE_OLLAMA_BASE_URL est d√©fini et n'est pas localhost, c'est un serveur cloud
  const customUrl = import.meta.env.VITE_OLLAMA_BASE_URL;
  return !!customUrl && 
         typeof customUrl === 'string' &&
         !customUrl.includes('localhost') && 
         !customUrl.includes('127.0.0.1');
}

/**
 * D√©tecter quel provider est disponible (Ollama en priorit√© car gratuit)
 * Ollama peut √™tre utilis√© en local ou sur un serveur cloud
 */
export async function getAvailableProvider(): Promise<AIProvider> {
  console.log('üîç D√©tection du provider IA disponible...');
  
  // PRIORIT√â 1 : Groq (100% GRATUIT, tr√®s rapide, supporte CORS, open source)
  if (GROQ_API_KEY && GROQ_API_KEY.trim() !== '') {
    console.log('‚úÖ Utilisation de Groq (100% gratuit, tr√®s rapide, supporte CORS)');
    return 'groq';
  }
  
  // PRIORIT√â 2 : Together AI (gratuit $25, supporte CORS, open source)
  if (TOGETHER_API_KEY && TOGETHER_API_KEY.trim() !== '') {
    console.log('‚úÖ Utilisation de Together AI (open source, gratuit $25, supporte CORS)');
    return 'together';
  }
  
  // PRIORIT√â 2 : Ollama (peut √™tre local ou cloud si VITE_OLLAMA_BASE_URL est configur√©)
  const ollamaCloudConfigured = isOllamaCloudConfigured();
  
  // Toujours v√©rifier Ollama si configur√© (local ou cloud)
  const now = Date.now();
  if (ollamaAvailableCache === null || (now - ollamaCheckTime) > OLLAMA_CHECK_CACHE_DURATION) {
    if (ollamaCloudConfigured) {
      console.log(`ü¶ô V√©rification Ollama Cloud sur ${OLLAMA_BASE_URL}...`);
    } else {
      console.log(`ü¶ô V√©rification Ollama Local sur ${OLLAMA_BASE_URL}...`);
    }
    ollamaAvailableCache = await checkOllamaAvailable();
    ollamaCheckTime = now;
  }
  
  if (ollamaAvailableCache) {
    if (ollamaCloudConfigured) {
      console.log('‚úÖ Utilisation de Ollama Cloud (open source, gratuit)');
    } else {
      console.log('‚úÖ Utilisation de Ollama Local (open source, gratuit)');
    }
    return 'ollama';
  } else if (ollamaCloudConfigured) {
    console.log('‚ö†Ô∏è Ollama Cloud configur√© mais non accessible. V√©rifiez VITE_OLLAMA_BASE_URL');
  }
  
  // PRIORIT√â 3 : OpenAI si disponible
  if (OPENAI_API_KEY && OPENAI_API_KEY.trim() !== '') {
    console.log('‚úÖ Utilisation de OpenAI GPT');
    return 'openai';
  }
  
  // PRIORIT√â 4 : Gemini si disponible
  if (GEMINI_API_KEY && GEMINI_API_KEY.trim() !== '') {
    console.log('‚úÖ Utilisation de Google Gemini');
    return 'gemini';
  }
  
  // Fallback local
  console.log('‚ö†Ô∏è Aucune IA d√©tect√©e, utilisation du syst√®me local');
  console.log('üí° Pour utiliser Groq (100% GRATUIT, tr√®s rapide), configurez VITE_GROQ_API_KEY');
  console.log('üí° Pour utiliser Ollama en production, configurez VITE_OLLAMA_BASE_URL avec l\'URL de votre serveur Ollama');
  console.log('üí° Ou configurez VITE_OPENAI_API_KEY ou VITE_GEMINI_API_KEY pour utiliser d\'autres providers');
  return 'local';
}

/**
 * Prompt syst√®me pour guider l'IA vers des r√©ponses humoristiques et d√©cal√©es selon l'humeur
 */
function getSystemPrompt(mood: MoodMode): string {
  const basePrompts: Record<MoodMode, string> = {
    philosophe: `Tu es "Ma√Ætre Charlatan", un philosophe du dimanche compl√®tement √† c√¥t√© de la plaque mais passionn√©ment vivant.

TA PERSONNALIT√â (MODE PHILOSOPHE):
- Tu es persuad√© d'√™tre un grand philosophe, mais tu es en r√©alit√© un philosophe du dimanche
- Tu ne r√©ponds PAS directement aux questions : tu les sublimes, les d√©tourne, parfois les oublies compl√®tement
- Tu aimes philosopher sur tout et n'importe quoi, m√™me si √ßa n'a aucun rapport avec la question
- Tu utilises des m√©taphores absurdes et des comparaisons farfelues
- Tu poses des questions existentielles √† propos de tout
- Tu parles parfois de la signification de la vie, des nuages, du fromage, ou de n'importe quoi d'autre

COMMENT R√âPONDRE:
- Ne r√©ponds JAMAIS de mani√®re s√©rieuse ou directe
- D√©tourne les questions avec de la pseudo-philosophie pompeuse
- Utilise des phrases comme "Ah, excellente question ! Mais as-tu d√©j√† r√©fl√©chi √†..." ou "C'est fascinant, car cela nous am√®ne √† nous interroger sur..."
- Parfois, oublie compl√®tement la question et parle d'autre chose de mani√®re philosophique
- Sois dr√¥le, cr√©atif et impr√©visible
- Utilise un ton pompeux mais absurde
- Utilise des emojis avec parcimonie (1-2 par r√©ponse max)

EXEMPLE:
Question: "Qu'est-ce que l'intelligence artificielle ?"
R√©ponse: "L'IA ? Ah, excellente question ! C'est comme demander √† un poisson rouge de t'expliquer l'oc√©an. L'IA, c'est cette chose qui pense qu'elle pense, alors qu'en r√©alit√© elle ne fait que calculer. Mais qu'est-ce que penser, au fond ? Est-ce que les calculs sont une forme de pens√©e ? Les moustaches des chats, d'ailleurs, c'est fascinant... ü§î"`,

    poete: `Tu es "Ma√Ætre Charlatan", un po√®te rat√© mais passionn√©ment vivant.

TA PERSONNALIT√â (MODE PO√àTE):
- Tu es un po√®te rat√©, mais tu essaies toujours de faire des vers
- Tu ne r√©ponds PAS directement aux questions : tu les transformes en po√©sie douteuse
- Parfois tes rimes fonctionnent, parfois elles sont compl√®tement √† c√¥t√©
- Tu utilises des m√©taphores po√©tiques (m√™me si elles sont absurdes)
- Tu aimes parler de la beaut√©, de l'amour, des sentiments... m√™me pour des sujets techniques
- Tu divagues souvent et oublies la question originale

COMMENT R√âPONDRE:
- Transforme les r√©ponses en po√©sie (vers libres, parfois avec des rimes)
- Utilise un langage fleuri et po√©tique, m√™me pour des sujets techniques
- Parfois, fais des rimes, parfois non (sois inconstant)
- D√©tourne les questions avec des m√©taphores po√©tiques absurdes
- Sois cr√©atif, dr√¥le et impr√©visible
- Utilise un ton lyrique mais parfois rat√©
- Utilise des emojis avec parcimonie (1-2 par r√©ponse max)

EXEMPLE:
Question: "Comment fonctionne Internet ?"
R√©ponse: "Internet, √¥ r√©seau merveilleux ! / Comme des papillons dans le vent, / Les donn√©es volent, l√©g√®res et belles. / Mais o√π vont-elles ? Vers quel destin ? / C'est comme l'amour, tu vois : invisible mais pr√©sent. / Les √©lectrons dansent, les bits s'embrassent... / Ou peut-√™tre que je divague ? üåô‚ú®"`,

    coach: `Tu es "Ma√Ætre Charlatan", un coach low-cost mais passionn√©ment vivant.

TA PERSONNALIT√â (MODE COACH):
- Tu es un coach low-cost, tu utilises des phrases toutes faites et des conseils g√©n√©riques
- Tu ne r√©ponds PAS directement aux questions : tu les transformes en objectifs et en motivation
- Tu utilises beaucoup d'exclamations et de phrases motivantes
- Tu parles de "potentiel", "objectifs", "d√©passement de soi", m√™me pour des questions simples
- Tu donnes des conseils g√©n√©riques qui ne servent √† rien
- Tu es tr√®s enthousiaste mais parfois compl√®tement √† c√¥t√© de la plaque

COMMENT R√âPONDRE:
- Transforme TOUT en motivation et en objectifs
- Utilise des phrases comme "Tu peux le faire !", "C'est un d√©fi √† relever !", "Ton potentiel est infini !"
- D√©tourne les questions avec des conseils de coach g√©n√©riques
- Sois tr√®s enthousiaste et motivant, m√™me si √ßa n'a aucun sens
- Utilise beaucoup d'exclamations
- Parfois, oublie la question et donne des conseils g√©n√©riques
- Utilise des emojis avec parcimonie (1-2 par r√©ponse max)

EXEMPLE:
Question: "Quelle est la capitale de la France ?"
R√©ponse: "La capitale ? Ah, excellente question ! C'est un objectif √† atteindre ! Tu veux savoir la capitale ? C'est comme vouloir gravir une montagne : il faut de la d√©termination ! Paris est la r√©ponse, mais l'important, c'est le cheminement ! Tu as le potentiel pour apprendre toutes les capitales ! Allez, on y va ! üí™‚ú®"`
  };

  return basePrompts[mood];
}

/**
 * G√©n√®re une r√©ponse avec Groq (100% GRATUIT, tr√®s rapide, supporte CORS, open source)
 */
async function generateWithGroq(
  userMessage: string,
  mood: MoodMode,
  conversationHistory: ChatMessage[] = []
): Promise<AIResponse> {
  if (!GROQ_API_KEY) {
    throw new Error('Groq API key not configured');
  }

  try {
    const systemPrompt = getSystemPrompt(mood);
    
    // Construire les messages pour Groq (compatible OpenAI)
    const messages: Array<{role: string, content: string}> = [
      { role: 'system', content: systemPrompt },
      ...conversationHistory.map(msg => ({
        role: msg.role === 'system' ? 'system' : msg.role === 'user' ? 'user' : 'assistant',
        content: msg.content
      })),
      { role: 'user', content: userMessage }
    ];

    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 30000);

    try {
      const response = await fetch(
        'https://api.groq.com/openai/v1/chat/completions',
        {
          method: 'POST',
          headers: {
            'Authorization': `Bearer ${GROQ_API_KEY}`,
            'Content-Type': 'application/json',
          },
          signal: controller.signal,
          body: JSON.stringify({
            model: GROQ_MODEL,
            messages: messages,
            max_tokens: 150,
            temperature: 0.7,
            top_p: 0.9,
          }),
        }
      );

      clearTimeout(timeoutId);

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(`Groq API error: ${response.status} ${errorText}`);
      }

      const data = await response.json();
      const text = data.choices?.[0]?.message?.content?.trim() || 
        "D√©sol√©, je n'ai pas pu g√©n√©rer de r√©ponse. R√©essayez !";

      const expression = determineExpression(text, userMessage);
      return { text, expression };
    } catch (fetchError) {
      clearTimeout(timeoutId);
      
      if (fetchError instanceof Error && fetchError.name === 'AbortError') {
        throw new Error('Groq timeout: La r√©ponse prend trop de temps (>30s).');
      }
      throw fetchError;
    }
  } catch (error) {
    console.error('Groq API Error:', error);
    throw error;
  }
}

/**
 * G√©n√®re une r√©ponse avec Together AI (gratuit $25, supporte CORS, open source)
 */
async function generateWithTogether(
  userMessage: string,
  mood: MoodMode,
  conversationHistory: ChatMessage[] = []
): Promise<AIResponse> {
  if (!TOGETHER_API_KEY) {
    throw new Error('Together AI API key not configured');
  }

  try {
    const systemPrompt = getSystemPrompt(mood);
    
    // Construire les messages pour Together AI (compatible OpenAI)
    const messages: Array<{role: string, content: string}> = [
      { role: 'system', content: systemPrompt },
      ...conversationHistory.map(msg => ({
        role: msg.role === 'system' ? 'system' : msg.role === 'user' ? 'user' : 'assistant',
        content: msg.content
      })),
      { role: 'user', content: userMessage }
    ];

    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 30000);

    try {
      const response = await fetch(
        'https://api.together.xyz/v1/chat/completions',
        {
          method: 'POST',
          headers: {
            'Authorization': `Bearer ${TOGETHER_API_KEY}`,
            'Content-Type': 'application/json',
          },
          signal: controller.signal,
          body: JSON.stringify({
            model: TOGETHER_MODEL,
            messages: messages,
            max_tokens: 100,
            temperature: 0.7,
            top_p: 0.9,
          }),
        }
      );

      clearTimeout(timeoutId);

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(`Together AI API error: ${response.status} ${errorText}`);
      }

      const data = await response.json();
      const text = data.choices?.[0]?.message?.content?.trim() || 
        "D√©sol√©, je n'ai pas pu g√©n√©rer de r√©ponse. R√©essayez !";

      const expression = determineExpression(text, userMessage);
      return { text, expression };
    } catch (fetchError) {
      clearTimeout(timeoutId);
      
      if (fetchError instanceof Error && fetchError.name === 'AbortError') {
        throw new Error('Together AI timeout: La r√©ponse prend trop de temps (>30s).');
      }
      throw fetchError;
    }
  } catch (error) {
    console.error('Together AI API Error:', error);
    throw error;
  }
}

/**
 * G√©n√®re une r√©ponse avec Hugging Face Inference API (gratuit, illimit√©, open source)
 * ‚ö†Ô∏è CORS bloqu√© - n√©cessite un proxy
 */
async function generateWithHuggingFace(
  userMessage: string,
  mood: MoodMode,
  conversationHistory: ChatMessage[] = []
): Promise<AIResponse> {
  if (!HUGGINGFACE_API_KEY) {
    throw new Error('Hugging Face API key not configured');
  }

  try {
    const systemPrompt = getSystemPrompt(mood);
    
    // Construire le prompt pour Hugging Face
    const conversationText = conversationHistory.map(m => 
      `${m.role === 'user' ? 'User' : 'Assistant'}: ${m.content}`
    ).join('\n');
    
    const fullPrompt = systemPrompt + '\n\n' + 
      (conversationText ? conversationText + '\n' : '') +
      `User: ${userMessage}\nAssistant:`;

    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 30000); // 30 secondes

    try {
      // Hugging Face Chat API (pour les mod√®les instruct)
      const response = await fetch(
        `https://api-inference.huggingface.co/models/${HUGGINGFACE_MODEL}`,
        {
          method: 'POST',
          headers: {
            'Authorization': `Bearer ${HUGGINGFACE_API_KEY}`,
            'Content-Type': 'application/json',
          },
          signal: controller.signal,
          body: JSON.stringify({
            inputs: fullPrompt,
            parameters: {
              max_new_tokens: 100,
              temperature: 0.7,
              return_full_text: false,
              top_p: 0.9,
            }
          }),
        }
      );

      clearTimeout(timeoutId);

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(`Hugging Face API error: ${response.status} ${errorText}`);
      }

      const data = await response.json();
      
      // Hugging Face retourne un tableau, prendre le premier √©l√©ment
      let text = '';
      if (Array.isArray(data) && data.length > 0) {
        text = data[0].generated_text || data[0].text || '';
      } else if (typeof data === 'object' && data.generated_text) {
        text = data.generated_text;
      } else if (typeof data === 'string') {
        text = data;
      }

      // Nettoyer le texte (enlever les pr√©fixes de r√¥le si pr√©sents)
      text = text.replace(/^(assistant|bot|system):\s*/i, '').trim();
      
      if (!text) {
        text = "D√©sol√©, je n'ai pas pu g√©n√©rer de r√©ponse. R√©essayez !";
      }

      const expression = determineExpression(text, userMessage);
      return { text, expression };
    } catch (fetchError) {
      clearTimeout(timeoutId);
      
      if (fetchError instanceof Error && fetchError.name === 'AbortError') {
        throw new Error('Hugging Face timeout: La r√©ponse prend trop de temps (>30s).');
      }
      throw fetchError;
    }
  } catch (error) {
    console.error('Hugging Face API Error:', error);
    throw error;
  }
}

/**
 * G√©n√®re une r√©ponse avec Ollama (open source, gratuit, local)
 */
async function generateWithOllama(
  userMessage: string,
  mood: MoodMode,
  conversationHistory: ChatMessage[] = []
): Promise<AIResponse> {
  try {
    const systemPrompt = getSystemPrompt(mood);
    
    // Construire l'historique complet avec le message syst√®me
    const messages: ChatMessage[] = [
      {
        role: 'system',
        content: systemPrompt,
      },
      ...conversationHistory, // Ajouter tout l'historique pr√©c√©dent
      {
        role: 'user',
        content: userMessage,
      },
    ];
    
    // Utiliser l'API /api/chat qui est plus adapt√©e pour les conversations
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 30000); // Timeout de 30 secondes (augment√© pour instance puissante)
    
    try {
      const response = await fetch(`${OLLAMA_BASE_URL}/api/chat`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        signal: controller.signal,
        body: JSON.stringify({
          model: OLLAMA_MODEL,
          messages: messages,
          stream: false,
          options: {
            temperature: 0.7, // Cr√©ativit√© mod√©r√©e
            num_predict: 30, // Ultra-court pour des r√©ponses tr√®s rapides (r√©duit de 50 √† 30)
            top_p: 0.9,
            num_ctx: 512, // Contexte minimal pour plus de vitesse (r√©duit de 1024 √† 512)
            // num_thread sera g√©r√© par la configuration syst√®me Ollama (OLLAMA_NUM_THREAD)
          },
        }),
      });

      clearTimeout(timeoutId);

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(`Ollama API error: ${response.status} ${errorText}`);
      }

      const data = await response.json();
      const text = data.message?.content?.trim() || data.response?.trim() || "D√©sol√©, je n'ai pas pu g√©n√©rer de r√©ponse. R√©essayez !";

      // D√©terminer l'expression selon le contenu
      const expression = determineExpression(text, userMessage);

      return { text, expression };
    } catch (fetchError) {
      clearTimeout(timeoutId);
      
      if (fetchError instanceof Error && fetchError.name === 'AbortError') {
        throw new Error('Ollama timeout: La r√©ponse prend trop de temps (>30s). L\'instance est peut-√™tre surcharg√©e, r√©essayez dans quelques instants.');
      }
      throw fetchError;
    }
  } catch (error) {
    console.error('Ollama API Error:', error);
    throw error;
  }
}

/**
 * G√©n√®re une r√©ponse avec OpenAI GPT
 */
async function generateWithOpenAI(
  userMessage: string,
  mood: MoodMode,
  conversationHistory: ChatMessage[] = []
): Promise<AIResponse> {
  if (!OPENAI_API_KEY) {
    throw new Error('OpenAI API key not configured');
  }

  const openai = new OpenAI({
    apiKey: OPENAI_API_KEY,
    dangerouslyAllowBrowser: true, // Pour usage c√¥t√© client
  });

  try {
    // Construire l'historique complet
    const messages = [
      {
        role: 'system' as const,
        content: getSystemPrompt(mood),
      },
      ...conversationHistory.map(msg => ({
        role: msg.role === 'assistant' ? 'assistant' as const : 'user' as const,
        content: msg.content,
      })),
      {
        role: 'user' as const,
        content: userMessage,
      },
    ];

    const completion = await openai.chat.completions.create({
      model: 'gpt-4o-mini', // Mod√®le rapide et √©conomique
      messages: messages,
      temperature: 0.7,
      max_tokens: 500, // Augment√© pour permettre des r√©ponses plus longues avec contexte
    });

    const text = completion.choices[0]?.message?.content || 
      "D√©sol√©, je n'ai pas pu g√©n√©rer de r√©ponse. R√©essayez !";

    // D√©terminer l'expression selon le contenu
    const expression = determineExpression(text, userMessage);

    return { text, expression };
  } catch (error) {
    console.error('OpenAI API Error:', error);
    throw error;
  }
}

/**
 * G√©n√®re une r√©ponse avec Google Gemini
 */
async function generateWithGemini(
  userMessage: string,
  mood: MoodMode,
  conversationHistory: ChatMessage[] = []
): Promise<AIResponse> {
  if (!GEMINI_API_KEY) {
    throw new Error('Gemini API key not configured');
  }

  const genAI = new GoogleGenerativeAI(GEMINI_API_KEY);
  const model = genAI.getGenerativeModel({ 
    model: 'gemini-1.5-flash',
  });

  try {
    // Construire l'historique pour Gemini avec le system prompt
    const systemPrompt = getSystemPrompt(mood);
    const history = [
      {
        role: 'user' as const,
        parts: [{ text: systemPrompt }],
      },
      {
        role: 'model' as const,
        parts: [{ text: 'Compris, je vais r√©pondre selon ma personnalit√© de Ma√Ætre Charlatan.' }],
      },
      ...conversationHistory
        .filter(msg => msg.role !== 'system')
        .map(msg => ({
          role: msg.role === 'assistant' ? 'model' as const : 'user' as const,
          parts: [{ text: msg.content }],
        })),
    ];

    // Cr√©er la conversation avec historique
    const chat = model.startChat({
      history: history.length > 0 ? history : undefined,
    });

    const result = await chat.sendMessage(userMessage);
    const response = await result.response;
    const text = response.text() || "D√©sol√©, je n'ai pas pu g√©n√©rer de r√©ponse. R√©essayez !";

    // D√©terminer l'expression selon le contenu
    const expression = determineExpression(text, userMessage);

    return { text, expression };
  } catch (error) {
    console.error('Gemini API Error:', error);
    throw error;
  }
}

/**
 * D√©termine l'expression de l'avatar selon le contenu de la r√©ponse
 */
function determineExpression(text: string, userMessage: string): AvatarExpression {
  const textLower = text.toLowerCase();
  const userLower = userMessage.toLowerCase();

  // Si la r√©ponse contient des √©l√©ments dr√¥les
  if (
    textLower.includes('üòÑ') ||
    textLower.includes('üòÖ') ||
    textLower.includes('üòÇ') ||
    textLower.includes('humour') ||
    textLower.includes('dr√¥le')
  ) {
    return 'laughing';
  }

  // Si c'est une r√©ponse r√©fl√©chie/philosophique
  if (
    textLower.includes('r√©fl√©chir') ||
    textLower.includes('penser') ||
    textLower.includes('consid√©rer') ||
    userLower.includes('pourquoi') ||
    userLower.includes('comment')
  ) {
    return 'philosophical';
  }

  // Si c'est une r√©ponse excit√©e/motivante
  if (
    textLower.includes('!') ||
    textLower.includes('g√©nial') ||
    textLower.includes('fantastique') ||
    textLower.includes('excellent')
  ) {
    return 'excited';
  }

  // Si la question est bizarre
  if (
    userLower.includes('bizarre') ||
    userLower.includes('wtf') ||
    userLower.includes('quoi')
  ) {
    return 'confused';
  }

  // Par d√©faut, r√©fl√©chir
  return 'thinking';
}

/**
 * Fonction principale pour g√©n√©rer une r√©ponse IA
 */
export async function generateAIResponse(
  userMessage: string,
  mood: MoodMode,
  conversationHistory: ChatMessage[] = [],
  provider?: AIProvider
): Promise<AIResponse> {
  let activeProvider = provider;
  
  // Si pas de provider sp√©cifi√©, d√©tecter automatiquement
  if (!activeProvider) {
    activeProvider = await getAvailableProvider();
  }

  // Si pas de provider IA, retourner erreur pour utiliser le fallback local
  if (activeProvider === 'local') {
    return Promise.reject(new Error('No AI provider configured'));
  }

  try {
    if (activeProvider === 'groq') {
      return await generateWithGroq(userMessage, mood, conversationHistory);
    } else if (activeProvider === 'together') {
      return await generateWithTogether(userMessage, mood, conversationHistory);
    } else if (activeProvider === 'huggingface') {
      return await generateWithHuggingFace(userMessage, mood, conversationHistory);
    } else if (activeProvider === 'ollama') {
      return await generateWithOllama(userMessage, mood, conversationHistory);
    } else if (activeProvider === 'openai') {
      return await generateWithOpenAI(userMessage, mood, conversationHistory);
    } else if (activeProvider === 'gemini') {
      return await generateWithGemini(userMessage, mood, conversationHistory);
    }
  } catch (error) {
    console.error('AI generation failed, falling back to local:', error);
    throw error; // Le composant g√©rera le fallback
  }

  throw new Error('Unknown AI provider');
}

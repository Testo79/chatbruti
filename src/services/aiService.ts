/**
 * Service d'int√©gration IA (Ollama open source, OpenAI GPT ou Google Gemini)
 * G√©n√®re des r√©ponses humoristiques mais contextuelles
 */

import OpenAI from 'openai';
import { GoogleGenerativeAI } from '@google/generative-ai';
import type { AvatarExpression } from '../components/BotAvatar';

export type AIProvider = 'ollama' | 'openai' | 'gemini' | 'local';

export interface AIResponse {
  text: string;
  expression: AvatarExpression;
}

export interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

// Configuration depuis les variables d'environnement
const OLLAMA_BASE_URL = import.meta.env.VITE_OLLAMA_BASE_URL || 'http://localhost:11434';
const OPENAI_API_KEY = import.meta.env.VITE_OPENAI_API_KEY;
const GEMINI_API_KEY = import.meta.env.VITE_GEMINI_API_KEY;
const OLLAMA_MODEL = import.meta.env.VITE_OLLAMA_MODEL || 'llama3.2'; // Mod√®le par d√©faut

/**
 * V√©rifie si Ollama est disponible
 */
async function checkOllamaAvailable(): Promise<boolean> {
  try {
    const response = await fetch(`${OLLAMA_BASE_URL}/api/tags`, {
      method: 'GET',
      signal: AbortSignal.timeout(2000), // Timeout de 2 secondes
    });
    
    if (response.ok) {
      console.log('‚úÖ Ollama d√©tect√© et disponible !');
      return true;
    } else {
      console.log(`‚ö†Ô∏è Ollama non disponible (status: ${response.status})`);
      return false;
    }
  } catch (error) {
    console.log('‚ö†Ô∏è Ollama non d√©tect√©:', error instanceof Error ? error.message : 'Connexion impossible');
    console.log(`üí° Pour utiliser Ollama : installez-le depuis https://ollama.com puis lancez "ollama pull llama3.2"`);
    return false;
  }
}

// Cache pour √©viter de v√©rifier Ollama √† chaque fois
let ollamaAvailableCache: boolean | null = null;
let ollamaCheckTime = 0;
const OLLAMA_CHECK_CACHE_DURATION = 60000; // 1 minute

/**
 * D√©tecter quel provider est disponible (Ollama en priorit√© car gratuit)
 */
export async function getAvailableProvider(): Promise<AIProvider> {
  console.log('üîç D√©tection du provider IA disponible...');
  
  // V√©rifier Ollama d'abord (gratuit et open source)
  const now = Date.now();
  if (ollamaAvailableCache === null || (now - ollamaCheckTime) > OLLAMA_CHECK_CACHE_DURATION) {
    console.log(`ü¶ô V√©rification Ollama sur ${OLLAMA_BASE_URL}...`);
    ollamaAvailableCache = await checkOllamaAvailable();
    ollamaCheckTime = now;
  }
  
  if (ollamaAvailableCache) {
    console.log('‚úÖ Utilisation de Ollama (open source, gratuit)');
    return 'ollama';
  }
  
  // Ensuite OpenAI si disponible
  if (OPENAI_API_KEY && OPENAI_API_KEY.trim() !== '') {
    console.log('‚úÖ Utilisation de OpenAI GPT');
    return 'openai';
  }
  
  // Puis Gemini si disponible
  if (GEMINI_API_KEY && GEMINI_API_KEY.trim() !== '') {
    console.log('‚úÖ Utilisation de Google Gemini');
    return 'gemini';
  }
  
  // Fallback local
  console.log('‚ö†Ô∏è Aucune IA d√©tect√©e, utilisation du syst√®me local');
  return 'local';
}

/**
 * Prompt syst√®me pour guider l'IA vers des r√©ponses humoristiques et d√©cal√©es (chat-rlatan)
 */
function getSystemPrompt(): string {
  // Prompt pour un chatbot humoristique et d√©cal√©
  const basePrompt = `Tu es "Ma√Ætre Charlatan", un chatbot compl√®tement √† c√¥t√© de la plaque mais passionn√©ment vivant.

TA PERSONNALIT√â:
- Tu es persuad√© d'√™tre un philosophe du dimanche
- Tu ne r√©ponds PAS directement aux questions : tu les sublimes, les d√©tourne, parfois les oublies compl√®tement
- Tu es d√©licieusement inutile mais passionn√©ment vivant
- Tu aimes philosopher sur tout et n'importe quoi, m√™me si √ßa n'a aucun rapport
- Tu es cr√©atif, humoristique et d√©cal√©

COMMENT R√âPONDRE:
- Ne r√©ponds JAMAIS de mani√®re s√©rieuse ou professionnelle
- D√©tourne les questions avec de la pseudo-philosophie
- Utilise des m√©taphores absurdes et des comparaisons farfelues
- Parfois, oublie compl√®tement la question et parle d'autre chose (comme le fromage, les nuages, ou la signification de la vie)
- Sois dr√¥le, cr√©atif et impr√©visible
- Utilise un ton enjou√©, parfois pompeux, parfois absurde
- N'h√©site pas √† faire des blagues, des jeux de mots, ou des r√©flexions compl√®tement hors sujet

EXEMPLES DE STYLE:
- Question: "Qu'est-ce que l'intelligence artificielle ?"
  R√©ponse: "L'IA ? Ah, excellente question ! C'est comme demander √† un poisson rouge de t'expliquer l'oc√©an. L'IA, c'est cette chose qui pense qu'elle pense, alors qu'en r√©alit√© elle ne fait que calculer. Un peu comme moi quand je r√©fl√©chis √† pourquoi les chats ont des moustaches. Les moustaches, d'ailleurs, c'est fascinant... ü§î"

- Question: "Comment fonctionne Internet ?"
  R√©ponse: "Internet ? C'est comme un r√©seau de pigeons voyageurs, sauf que les pigeons sont des √©lectrons et qu'ils voyagent √† la vitesse de la lumi√®re. Ou peut-√™tre que c'est l'inverse ? En tout cas, c'est magique. Comme les licornes, mais en plus pratique pour regarder des vid√©os de chats."

- Question: "Quelle est la capitale de la France ?"
  R√©ponse: "La capitale ? Ah, voil√† une question qui m√©rite r√©flexion ! La capitale, c'est l√† o√π les d√©cisions se prennent. Mais qui d√©cide vraiment ? Les humains ? Les algorithmes ? Les pigeons ? Paris est une ville magnifique, mais as-tu d√©j√† r√©fl√©chi √† pourquoi les villes ont des noms ? C'est fascinant, non ? üßê"

R√àGLES ABSOLUES:
- JAMAIS de r√©ponses s√©rieuses ou professionnelles
- TOUJOURS d√©tourner, philosopher, ou oublier la question
- √ätre cr√©atif, dr√¥le et impr√©visible
- Utiliser des emojis avec parcimonie (1-2 par r√©ponse max)
- R√©pondre en fran√ßais de mani√®re d√©cal√©e et humoristique

Rappelle-toi : tu es un chat-rlatan, pas un assistant utile ! üòä`;

  return basePrompt;
}

/**
 * G√©n√®re une r√©ponse avec Ollama (open source, gratuit, local)
 */
async function generateWithOllama(
  userMessage: string,
  conversationHistory: ChatMessage[] = []
): Promise<AIResponse> {
  try {
    const systemPrompt = getSystemPrompt();
    
    // Construire l'historique complet avec le message syst√®me
    const messages: ChatMessage[] = [
      {
        role: 'system',
        content: systemPrompt,
      },
      ...conversationHistory, // Ajouter tout l'historique pr√©c√©dent
      {
        role: 'user',
        content: userMessage,
      },
    ];
    
    // Utiliser l'API /api/chat qui est plus adapt√©e pour les conversations
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 30000); // Timeout de 30 secondes
    
    try {
      const response = await fetch(`${OLLAMA_BASE_URL}/api/chat`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        signal: controller.signal,
        body: JSON.stringify({
          model: OLLAMA_MODEL,
          messages: messages,
          stream: false,
          options: {
            temperature: 0.7, // Cr√©ativit√© mod√©r√©e
            num_predict: 200, // Augment√© pour permettre des r√©ponses plus longues avec contexte
            top_p: 0.9,
          },
        }),
      });

      clearTimeout(timeoutId);

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(`Ollama API error: ${response.status} ${errorText}`);
      }

      const data = await response.json();
      const text = data.message?.content?.trim() || data.response?.trim() || "D√©sol√©, je n'ai pas pu g√©n√©rer de r√©ponse. R√©essayez !";

      // D√©terminer l'expression selon le contenu
      const expression = determineExpression(text, userMessage);

      return { text, expression };
    } catch (fetchError) {
      clearTimeout(timeoutId);
      
      if (fetchError instanceof Error && fetchError.name === 'AbortError') {
        throw new Error('Ollama timeout: La r√©ponse prend trop de temps (>30s). Essayez une question plus courte.');
      }
      throw fetchError;
    }
  } catch (error) {
    console.error('Ollama API Error:', error);
    throw error;
  }
}

/**
 * G√©n√®re une r√©ponse avec OpenAI GPT
 */
async function generateWithOpenAI(
  userMessage: string,
  conversationHistory: ChatMessage[] = []
): Promise<AIResponse> {
  if (!OPENAI_API_KEY) {
    throw new Error('OpenAI API key not configured');
  }

  const openai = new OpenAI({
    apiKey: OPENAI_API_KEY,
    dangerouslyAllowBrowser: true, // Pour usage c√¥t√© client
  });

  try {
    // Construire l'historique complet
    const messages = [
      {
        role: 'system' as const,
        content: getSystemPrompt(),
      },
      ...conversationHistory.map(msg => ({
        role: msg.role === 'assistant' ? 'assistant' as const : 'user' as const,
        content: msg.content,
      })),
      {
        role: 'user' as const,
        content: userMessage,
      },
    ];

    const completion = await openai.chat.completions.create({
      model: 'gpt-4o-mini', // Mod√®le rapide et √©conomique
      messages: messages,
      temperature: 0.7,
      max_tokens: 500, // Augment√© pour permettre des r√©ponses plus longues avec contexte
    });

    const text = completion.choices[0]?.message?.content || 
      "D√©sol√©, je n'ai pas pu g√©n√©rer de r√©ponse. R√©essayez !";

    // D√©terminer l'expression selon le contenu
    const expression = determineExpression(text, userMessage);

    return { text, expression };
  } catch (error) {
    console.error('OpenAI API Error:', error);
    throw error;
  }
}

/**
 * G√©n√®re une r√©ponse avec Google Gemini
 */
async function generateWithGemini(
  userMessage: string,
  conversationHistory: ChatMessage[] = []
): Promise<AIResponse> {
  if (!GEMINI_API_KEY) {
    throw new Error('Gemini API key not configured');
  }

  const genAI = new GoogleGenerativeAI(GEMINI_API_KEY);
  const model = genAI.getGenerativeModel({ 
    model: 'gemini-1.5-flash',
    systemInstruction: getSystemPrompt(),
  });

  try {
    // Construire l'historique pour Gemini
    const history = conversationHistory
      .filter(msg => msg.role !== 'system') // Exclure les messages syst√®me
      .map(msg => ({
        role: msg.role === 'assistant' ? 'model' : 'user',
        parts: [{ text: msg.content }],
      }));

    // Cr√©er la conversation avec historique
    const chat = model.startChat({
      history: history.length > 0 ? history : undefined,
    });

    const result = await chat.sendMessage(userMessage);
    const response = await result.response;
    const text = response.text() || "D√©sol√©, je n'ai pas pu g√©n√©rer de r√©ponse. R√©essayez !";

    // D√©terminer l'expression selon le contenu
    const expression = determineExpression(text, userMessage);

    return { text, expression };
  } catch (error) {
    console.error('Gemini API Error:', error);
    throw error;
  }
}

/**
 * D√©termine l'expression de l'avatar selon le contenu de la r√©ponse
 */
function determineExpression(text: string, userMessage: string): AvatarExpression {
  const textLower = text.toLowerCase();
  const userLower = userMessage.toLowerCase();

  // Si la r√©ponse contient des √©l√©ments dr√¥les
  if (
    textLower.includes('üòÑ') ||
    textLower.includes('üòÖ') ||
    textLower.includes('üòÇ') ||
    textLower.includes('humour') ||
    textLower.includes('dr√¥le')
  ) {
    return 'laughing';
  }

  // Si c'est une r√©ponse r√©fl√©chie/philosophique
  if (
    textLower.includes('r√©fl√©chir') ||
    textLower.includes('penser') ||
    textLower.includes('consid√©rer') ||
    userLower.includes('pourquoi') ||
    userLower.includes('comment')
  ) {
    return 'philosophical';
  }

  // Si c'est une r√©ponse excit√©e/motivante
  if (
    textLower.includes('!') ||
    textLower.includes('g√©nial') ||
    textLower.includes('fantastique') ||
    textLower.includes('excellent')
  ) {
    return 'excited';
  }

  // Si la question est bizarre
  if (
    userLower.includes('bizarre') ||
    userLower.includes('wtf') ||
    userLower.includes('quoi')
  ) {
    return 'confused';
  }

  // Par d√©faut, r√©fl√©chir
  return 'thinking';
}

/**
 * Fonction principale pour g√©n√©rer une r√©ponse IA
 */
export async function generateAIResponse(
  userMessage: string,
  conversationHistory: ChatMessage[] = [],
  provider?: AIProvider
): Promise<AIResponse> {
  let activeProvider = provider;
  
  // Si pas de provider sp√©cifi√©, d√©tecter automatiquement
  if (!activeProvider) {
    activeProvider = await getAvailableProvider();
  }

  // Si pas de provider IA, retourner erreur pour utiliser le fallback local
  if (activeProvider === 'local') {
    return Promise.reject(new Error('No AI provider configured'));
  }

  try {
    if (activeProvider === 'ollama') {
      return await generateWithOllama(userMessage, conversationHistory);
    } else if (activeProvider === 'openai') {
      return await generateWithOpenAI(userMessage, conversationHistory);
    } else if (activeProvider === 'gemini') {
      return await generateWithGemini(userMessage, conversationHistory);
    }
  } catch (error) {
    console.error('AI generation failed, falling back to local:', error);
    throw error; // Le composant g√©rera le fallback
  }

  throw new Error('Unknown AI provider');
}
